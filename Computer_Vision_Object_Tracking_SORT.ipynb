{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Computer Vision-Object Tracking-SORT & YOLO Algorithms**"
      ],
      "metadata": {
        "id": "5DWe4_-uIp2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second: Tracking by SORT"
      ],
      "metadata": {
        "id": "8PhxULiPI0Ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iDcR63SSIjDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2129b8-0807-4103-ef8d-d268d1b9429b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m174.1/178.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=db8afa69374888476106197ad9b88ae2b54de181691a187abbff878d84ab96e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install filterpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AmirMansurian/SORT\n",
        "%cd SORT/\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights -O config/yolov3.weights"
      ],
      "metadata": {
        "id": "hTXSLE6uK7J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07edfbf2-bd99-4bcd-f070-b9abed8e646b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SORT'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 27 (delta 3), reused 22 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (27/27), 10.76 MiB | 16.30 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "/content/SORT\n",
            "--2024-07-10 12:48:58--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘config/yolov3.weights’\n",
            "\n",
            "config/yolov3.weigh 100%[===================>] 236.52M  40.3MB/s    in 5.7s    \n",
            "\n",
            "2024-07-10 12:49:04 (41.1 MB/s) - ‘config/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "import imageio\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/SORT')\n",
        "\n",
        "from sort import *\n",
        "from sort import iou\n",
        "\n",
        "import cv2\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "31f3bitLLofg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccfa986-c551-4b7c-857a-b6cb2cf92e35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/SORT/sort.py:28: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def iou(bb_test,bb_gt):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object Detection"
      ],
      "metadata": {
        "id": "JvFT_NT9MAgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path='config/yolov3.cfg'\n",
        "weights_path='config/yolov3.weights'\n",
        "class_path='config/coco.names'\n",
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.4\n",
        "\n",
        "# Load model and weights\n",
        "model = Darknet(config_path, img_size=img_size) # Definition of detection model\n",
        "model.load_weights(weights_path) # Loading network weights\n",
        "model.cuda() # Using GPU\n",
        "model.eval() # The model is already trained and only needs to be in evaluation mode\n",
        "classes = utils.load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "metadata": {
        "id": "a5pvngptMFCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080efd5a-7076-433e-90ab-e3aac766a163"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image(img):\n",
        "    # Basic pre-processing on the image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "\n",
        "    # Image to tensor conversion for network input\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = Variable(image_tensor.type(Tensor))\n",
        "\n",
        "   # Using the model in evaluation mode and giving an image to the trained model\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "\n",
        "    return detections[0] # The output of the bounding boxes will be inside the image"
      ],
      "metadata": {
        "id": "xX15pOV-MdDD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertMillis(millseconds):\n",
        "    seconds, millseconds = divmod(millseconds, 1000)\n",
        "    minutes, seconds = divmod(seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    day, hours = divmod(hours, 24)\n",
        "    seconds = int(seconds + millseconds/10000)\n",
        "    return f\"{int(hours)}:{int(minutes)}:{int(seconds)}\""
      ],
      "metadata": {
        "id": "OKrkyUrUMqUV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracking"
      ],
      "metadata": {
        "id": "kNmJoTovrAaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOX3UtDUOX1V",
        "outputId": "d1d10b6b-b028-4e18-b905-244a7cbe209b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lap\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: lap\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp310-cp310-linux_x86_64.whl size=1628945 sha256=e1fb256e6dce0760bb67192d8204dc4c4b91636034786563ebe603f70a0791cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/42/2e/9dfe19270eea279d79e84767ff0d7b8082c3bf776cad00e83d\n",
            "Successfully built lap\n",
            "Installing collected packages: lap\n",
            "Successfully installed lap-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "videopath = '/content/SORT/video/traffic_1.gif'\n",
        "\n",
        "%pylab inline\n",
        "\n",
        "cmap = plt.get_cmap('tab20b')\n",
        "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "# Loading video\n",
        "cap = cv2.VideoCapture('/content/SORT/video/traffic_1.gif')\n",
        "\n",
        "\n",
        "mot_tracker = Sort() # Tracking model definition\n",
        "Frames = [] # To save the final video\n",
        "\n",
        "num_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "for ii in range(num_frame):\n",
        "    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "    time_report = convertMillis(timestamp)\n",
        "    ret, frame = cap.read() # Loading a frame of video\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pilimg = Image.fromarray(frame)\n",
        "    detections = detect_image(pilimg) # Giving the loaded frame to the detection network\n",
        "\n",
        "    img = np.array(pilimg)\n",
        "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "    unpad_h = img_size - pad_y\n",
        "    unpad_w = img_size - pad_x\n",
        "\n",
        "    if detections is not None:\n",
        "        tracked_objects = mot_tracker.update(detections.cpu()) # Sending the detection output to the tracking network\n",
        "\n",
        "        unique_labels = detections[:, -1].cpu().unique()\n",
        "        n_cls_preds = len(unique_labels)\n",
        "\n",
        "        ########## Displaying the bounding boxes of objects and their ID on each frame ###########\n",
        "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
        "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
        "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
        "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
        "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
        "\n",
        "            color = colors[int(obj_id) % len(colors)]\n",
        "            color = [i * 255 for i in color]\n",
        "            cls = classes[int(cls_pred)]\n",
        "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 1) # The bounding box around the object\n",
        "            cv2.rectangle(frame, (x1, y1-20), (x1+len(cls)*15, y1), color, -1) # A bounding box above each object to display the class type and object ID\n",
        "\n",
        "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1) # Text related to class name and ID\n",
        "\n",
        "    Frames.append(frame) # Save processed frames\n",
        "\n",
        "# Save the tracked video frames as a GIF\n",
        "output_path = 'tracked_video_ZEYNAB.gif'\n",
        "imageio.mimsave(output_path, Frames, fps=10)\n"
      ],
      "metadata": {
        "id": "NOkSuRtNANOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}